{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report(WeRateDogs Project)\n",
    "## By Niharika Jain\n",
    "### Date: 01/12/22\n",
    "\n",
    ">1. [INTRODUCTION](#introduction)\n",
    ">1. [GATHERING](#gathering)\n",
    ">1. [ASSESSING](#assessing)\n",
    ">1. [CLEANING](#cleaning)\n",
    ">1. [CONCLUSION](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='introduction'>INTRODUCTION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Final Project of the Wrangling unit, we are wrangling the tweet archive of Twitter user [@dog_rates](https://twitter.com/dog_rates), also known as [WeRateDogs](https://en.wikipedia.org/wiki/WeRateDogs). WeRateDog account has more than 4 million followers and this account rates people's dogs with a delightful observation about the dog. The focus areas of this project are gathering, assessing, and cleaning the collected data. The result would be analyse and visualize our wrangled data with alteast three (3) insights and one (1) visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='gathering'>GATHERING</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I this project I used 3 different data souces to collect information, which were turned into dataframe which I used for cleaning and visulaizatio.\n",
    "\n",
    "1. twitter-archive-enhanced.csv\n",
    "1. image-predictions.tsv\n",
    "1. tweet_json.txt\n",
    "\n",
    "<br>First file **twitter-archive-enhanced.csv** was given to me in Udacity Project file. I downloaded this file manullay, uploaded it and read the data using Pandas Dataframe in Jupyter Notebook.\n",
    "\n",
    "```\n",
    "twitter_archive = pd.read_csv('twitter-archive-enhanced.csv') \n",
    "```\n",
    "\n",
    "For **image-predictions.tsv**, Tsv file URL is given to me in Udacity Project file. With the help of Requests library I downloaded the tweet image prediction file.\n",
    "```\n",
    "url=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "tweet_image_request =requests.get(url)\n",
    "\n",
    "with open('image-predictions.tsv', mode = 'wb') as file:\n",
    "    file.write(tweet_image_request.content)\n",
    "\n",
    "image_predictions = pd.read_csv('image-predictions.tsv', sep = '\\t')\n",
    "```\n",
    "Last **tweet_json.txt** was bit complicated for me because it took almost4-5 days to fetch data from using Twitter API. I created twitter developer account and got my Access Keys. With the help of Tweepy Library and assess key, I can access Twitter API and get all data which I need for my project. I fetched and saved only tweet_id, reweet count and favorite count in file.\n",
    "```\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "access_secret = 'HIDDEN'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "tweet_ids = twitter_archive['tweet_id']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id ='assessing'>ASSESSING</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the gathering all 3 dataset successfully, I moved to assess data for quality and tidiness issues. I have to do this with visually and programitically. When I was checking data I found that most of the issuses are in the **twitter_archive** table which has the largest dataset. The **head(), tail(), sample(), info()** and **describe()** functions are frequently used to visually assess each dataset. \n",
    "```\n",
    "twitter_archive.haid(4)\n",
    "\n",
    "image_predictions.sample(4)\n",
    "\n",
    "tweet_json.tail()\n",
    "\n",
    "twitter_archive.info()\n",
    "\n",
    "tweet_json.describe()\n",
    "```\n",
    "While prgrammatically assessing the dataset **value_counts(),duplicated()** and **isnull()** functions were frequently used in this step.\n",
    "```\n",
    "print(\"Checking Duplicate Value of Name in Twitter Archive\\n\",twitter_archive.name.value_counts())\n",
    "\n",
    "tweet_json.retweet_count.value_counts()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id ='cleaning'>CLEANING</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of data wrangling is cleaning, in this step I had to use define, code and test format for each operations. Many functions were being used for this step to fix the following problems which we found in previous step:\n",
    "\n",
    "### Quality Issues\n",
    "##### Twitter Archieve:\n",
    "1. tweet_id column datatype need to be change to sting. (Also In Image Predictions Dataset)\n",
    "\n",
    "1. timestamp column datatype need to be change to datetime.\n",
    "\n",
    "1. As per requirement we don't need to analysis Retweet data so first we will remove all retweet from dataset after that we will remove all unnecessary columns.  \n",
    "> timestamp','in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp',\n",
    "'expanded_urls'\n",
    "\n",
    "1. In the source column full hyperlink tag information is available which is bit difficult to understand. We can replace hyperlink tag with Link name only.\n",
    "\n",
    "1.  \"Doggo', 'Floofer', 'Pipper', 'Puppo' columns contains None instead of NaN.With the help of Numpy convertung None Value to NaN.\n",
    "\n",
    "1. rating_numerator & rating_denominator values are incorrect.\n",
    "\n",
    "> As per requirement rating numerator should be between 10-15 and denominator should be 10. But in dataset rating_numerator value range is 0-1776 and denominator values are 0-170.\n",
    "> <br>Text column conatins rating for each dog. We need to extract correct value from Text column. I am doing this activity manually after exporting effected data in excel sheet.<br>\n",
    "\n",
    "### rating_denominator (Solution)\n",
    "><br>**Note:** After examining the spreadsheet, I could identify 3 error-types within the 18 entries with irregular denominators:\n",
    ">1. 13 entries had rating based on no of dogs\n",
    ">1. 5 entries had incorrect ratings\n",
    ">1. 4 entry had no rating at all\n",
    ">**Cleaning method:** I did not find how to correct programatically so I updated excel sheet manually by add new updated values in new column **\"new_rating_numerator\"** also updating denominator value to 10 under **\"new_rating_denominator\"** column.\n",
    ">1. Use a simple cross-multiplication method (numerator*10/denominator) to normalize the numerator.\n",
    ">1. Manually enter the ratings using text column which has correct data.\n",
    ">1. Manually enter the rating using the median which is 11.0\n",
    "\n",
    "\n",
    "### rating_numerator (Solution)\n",
    "> <br>**Note:** After examining the spreadsheet, I could identify 3 error-types within the 9 entries with irregular numerator:\n",
    ">1. 5 entries had rating based on no of dogs\n",
    ">1. 4 entries had incorrect ratings\n",
    ">**Cleaning method:** I did not find how to correct programatically so I updated excel sheet manually by add new updated values in new column **\"new_rating_numerator\"**.\n",
    ">1. Use a simple cross-multiplication method (numerator*10/denominator) to normalize the numerator.\n",
    ">1. Manually enter the ratings using text column which has correct data.\n",
    "\n",
    "1. More than 700 Dog Name is 'None' and More than 50 Dogs Name is 'a'\n",
    ">1. First I am replacing 'None' value to 'NaN'\n",
    ">1. Not able to figure out how to update \"a\" so leaving all values like this only.\n",
    "\n",
    "##### Image Predictions:\n",
    "1. P1, P2 and P3 have inconsisitent capital words and unnessary underscore instead of space.\n",
    "\n",
    "1. jpg_url have more than 60 entries with dupliacte values. So removing all duplicate values from dataset.\n",
    "\n",
    "1. p1 name is not descriptive. These are the breed of dogs so changing column name to 'breed'.\n",
    "\n",
    "1. p1_conf, p1_dog, p2, p2_donf, p2_dog, p3, p3_conf, p3_dog columns are not required.\n",
    "\n",
    "### Tidiness Issues\n",
    "\n",
    "1. Need to combine 4 Columns(dog stages) and Make it one. (Twitter Archive)\n",
    "\n",
    "1. Join Tweet Json & Image Predictions datasets with the twitter_archive. (Tweet Json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cleaning process(gathered, assessed, and cleaned), I saved the master dataset to a CSV file named **twitter_archive_master.csv**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id ='conclusion'>CONCLUSION</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I faced 2-3 challenges while doing this project which was the major learning for me. \n",
    ">The first and the biggest challenge was when I could not get data through Twitter API. But after so many attempts, I finally found it.\n",
    "><br><br>The second was when I had to correct Numerator and Denominator values. I knew the correct value mentioned in the text column, somehow I could not get those values programmatically, so I chose to do those corrections manually in the excel sheet and uploaded the correct values in the dataset.\n",
    "><br><br>The final challenge was I could not correct Dog's Name. More than 600 Names in the Name column are defined as  \"None\" which stands incorrect. I did not find any way to fix those values, so I converted them as \"NaN\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
